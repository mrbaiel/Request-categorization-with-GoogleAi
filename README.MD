Первым мне надо было сгенерить фейк данные имитирующих реальных пользователей.Для начала я решил выполнить задачу с использованием библиотеки transformers4.49.0 от Hugging Face,
в итоге я смог добиться ответа но не цели потому что модель оказалась не достаточно обученной примеры из ответов (можно посмотреть в директории tests_with_nlp):

```{
    "668296": {
        "created_at": "2025-01-23 00:12:16",
        "user_id": "943193",
        "message": "преступы свешот, прествавлей Э пандерчимы ссл"
    },
    "668298": {
        "created_at": "2025-01-23 00:23:34",
        "user_id": "1328750",
        "message": "Аланнайных в на: ни жилепа Ожитносьрпои пско�"
    }
}
```

После решил взять api ключ для INTERCOM из того примера что вы мне скидывали, она оказалась не верной и решить проблему
с апишкой не удалось

Далее обратился к ClaudeAi и Openai и в самый последний момент получил ошибку сервера : "страна не поддерживается"

Далее решил проблему с впн но увидел что решение с Gemini2.0 лучше тем что там нету ограниченных запросов.
Разделил обработку на три файла: generate_requests, categorize_requests, counting_requests

generate_requests.py
Для генерации разбивается запросы на партии по 10 потому что для генерации одного запроса тратилось по 8,7 секунд.
Далее берутся из исходного json файла 10 записей, обрабатывается, сохраняется сразу в новый файл чтобы избежать проблем с ошибками(сервера и кода)
и удаляется эти 10 записей из исходного файла. Это обеспечило более устойчивое состояние работоспособности кода.

categorize_requests.py
Далее таким же партичным шаблоном категоризируются обращения которые сохраняются в новый файл.
Загружается данные из файла где уже есть поле "message", обрабатывает каждое сообщение, определяет его категорию с помощью модели генеративного языка,
сохраняет результаты в новый файл,
удаляет обработанные записи из исходного списка и обновляет конечный файл с оставшимися записями.


counting_requests.py
загружается данные из файла, обрабатываются категории, очищает их от лишних символов и пробелов,
подсчитывает количество каждой категории и сохраняет результаты в новый json файл .

файлы которые несут смысл(которых можно посмотреть): 
final_test.json - сегенирились обращения в поддержку
category_result_test.json - прошел категоризацию каждого обращения
result_final.json - тут количество соответсвующему категорию